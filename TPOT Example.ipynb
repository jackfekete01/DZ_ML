{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all packages \n",
    "import pickle\n",
    "import detritalpy.detritalFuncs as dFunc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from random import choices\n",
    "from random import randrange\n",
    "from random import randint\n",
    "import random\n",
    "import time\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dataset to train the TPOT Classifier on\n",
    "# Here, we select the fixed age mode 0.95-0.99 similarity interval\n",
    "cut_0 = pickle.load(open('0.95-0.99 - 0 Sources.p','rb'))\n",
    "cut_1 = pickle.load(open('0.95-0.99 - 1 Sources.p','rb'))\n",
    "cut_2 = pickle.load(open('0.95-0.99 - 2 Sources.p','rb'))\n",
    "cut_3 = pickle.load(open('0.95-0.99 - 3 Sources.p','rb'))\n",
    "cut_4 = pickle.load(open('0.95-0.99 - 4 Sources.p','rb'))\n",
    "cut_5 = pickle.load(open('0.95-0.99 - 5 Sources.p','rb'))\n",
    "cut_6 = pickle.load(open('0.95-0.99 - 6 Sources.p','rb'))\n",
    "cut_7 = pickle.load(open('0.95-0.99 - 7 Sources.p','rb'))\n",
    "init_data = {**cut_0,**cut_1,**cut_2,**cut_3,**cut_4,**cut_5,**cut_6,**cut_7}\n",
    "\n",
    "# We need to select a subset of the data to run the TPOT classifier on\n",
    "# Lets use 5 sources at 115 analyses\n",
    "\n",
    "analyses_number = 115\n",
    "source_number = 5\n",
    "source_nums = np.random.choice((np.arange(0,8)),source_number,replace=False)\n",
    "\n",
    "auto_data = {}\n",
    "for key in init_data:\n",
    "    if key[0] in source_nums and key[1] == analyses_number:\n",
    "        auto_data[key] = init_data[key]\n",
    "\n",
    "# Format data into a dataframe\n",
    "ID_list = []\n",
    "KDE_list = []\n",
    "for entry in auto_data:\n",
    "    source_ID = entry[0]\n",
    "    for KDE in auto_data[entry]:\n",
    "        KDE_list.append(KDE)\n",
    "        ID_list.append(source_ID)\n",
    "final_df = pd.DataFrame()\n",
    "final_df['ID'] = ID_list\n",
    "final_df['KDE'] = KDE_list\n",
    "\n",
    "# Data preprocessing\n",
    "train_set,test_set = train_test_split(final_df,test_size = 0.2, random_state = randrange(42))\n",
    "train_KDE = list(train_set['KDE'])\n",
    "train_labels = list(train_set['ID'])\n",
    "test_KDE = list(test_set['KDE'])\n",
    "test_labels = list(test_set['ID'])\n",
    "enc = OrdinalEncoder()\n",
    "train_labels = np.array(train_labels)\n",
    "train_labels = train_labels.reshape(-1,1)\n",
    "train_labels = enc.fit_transform(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "test_labels = test_labels.reshape(-1,1)\n",
    "test_labels = enc.fit_transform(test_labels)\n",
    "train_KDE = np.array(train_KDE)\n",
    "test_KDE = np.array(test_KDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the TPOT classifier\n",
    "# Here, we specify the TPOT classifier hyperparamters\n",
    "# These hyperparamters represent a low end case that prioritizes speed over optimization\n",
    "# For better results, increase the generations and max_eval_time_mins\n",
    "\n",
    "# Number of iterations to run the pipeline optimization process (default=100)\n",
    "generations = 1\n",
    "# How much information TPOT communicates (0-3)\n",
    "verbosity = 2\n",
    "# The seed of the pseudo random number generator used in TPOT\n",
    "random_state = 42\n",
    "# Number of processes to use in parallel for evaluating pipelines during the TPOT optimization process\n",
    "n_jobs=-2\n",
    "# How many minutes TPOT has to evaluate a single pipeline\n",
    "max_eval_time_mins = 2\n",
    "# Function used to evaluate the quality of a given pipeline for the classification problem\n",
    "scoring = 'f1_weighted'\n",
    "\n",
    "tpot = TPOTClassifier(generations=generations, verbosity=verbosity, random_state=random_state,n_jobs=n_jobs,max_eval_time_mins = max_eval_time_mins,scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171fbda458ac4810ba98bd2e807325a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/200 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.48583333333333323\n",
      "\n",
      "Best pipeline: LogisticRegression(StandardScaler(input_matrix), C=15.0, dual=False, penalty=l2)\n",
      "0.22857142857142856\n"
     ]
    }
   ],
   "source": [
    "# Now we train and export the optimized pipeline\n",
    "model = tpot.fit(train_KDE, train_labels.ravel())\n",
    "print(tpot.score(test_KDE, test_labels.ravel()))\n",
    "tpot.export('Optimized TPOT Pipeline Example.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
